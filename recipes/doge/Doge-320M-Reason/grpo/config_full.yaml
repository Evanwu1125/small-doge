# Logging and Output arguments
log_level: info
logging_strategy: steps
logging_steps: 10
report_to:
- tensorboard
- wandb
save_strategy: steps
save_steps: 100
output_dir: data/Doge-320M-GRPO
overwrite_output_dir: true
push_to_hub: true
private: true
# token: <your-token>
hub_model_id: Doge-320M-GRPO
hub_strategy: every_save

# Model arguments
model_name_or_path: <your-hub-id>/Doge-320M-Distill
model_revision: main
trust_remote_code: True
torch_dtype: bfloat16

# Data training arguments
dataset_name: ./datasets/grpo_dataset
dataset_configs:
- all
max_prompt_length: 512
max_completion_length: 1024
packing: true
system_prompt: "As an assistant, you need to thoroughly explore the problem through precise thinking process before providing the final accurate solution. The thinking process includes Analysis, First, Second, Next, Reflection, Finally and Summarizing behavioral steps to develop a well-considered thought process. Please structure your response into two main sections: Thought and Solution. In the Thought section, detail your reasoning process using the specified format: <|begin_of_thought|> {**Analysis:**\\n\\n**First:**\\n\\n**Second:**\\n\\n**Next:**\\n\\n**Reflection:**\\n\\n**Finally:**\\n\\n**Summarizing:**} <|end_of_thought|>. The solution should remain a logical, accurate, concise expression style and detail necessary step needed to reach the conclusion, formatted as follows: <|begin_of_solution|> {**Solution:**} <|end_of_solution|>."

# GRPO trainer config
# If you are using vllm, please comment out preprocessing_num_workers and use num_processes
preprocessing_num_workers: 1 # Equal to the number of GPUs you are using
# Num processes is less by 1 as vLLM is using 1 GPU
# num_processes: 7
seed: 233
use_vllm: false
vllm_device: auto
vllm_gpu_memory_utilization: 0.7
reward_funcs:
- accuracy
- format
reward_weights:
- 1.0
- 1.0
do_train: true
max_steps: -1
num_train_epochs: 1
num_generations: 1 # Be consistent with preprocessing_num_workers or num_processes
per_device_train_batch_size: 1
do_eval: true
eval_strategy: steps
eval_steps: 100
per_device_eval_batch_size: 1
optim: adamw_torch
learning_rate: 2.0e-5
lr_scheduler_type: cosine_with_min_lr
lr_scheduler_kwargs:
  min_lr_rate: 0.1
warmup_ratio: 0.1
weight_decay: 0.01
gradient_accumulation_steps: 128
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false
max_grad_norm: 1.0
bf16: true
