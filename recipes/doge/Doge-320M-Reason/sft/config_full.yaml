# Logging and Output arguments
log_level: info
logging_strategy: steps
logging_steps: 10
report_to:
- tensorboard
- wandb
save_strategy: steps
save_steps: 100
output_dir: data/Doge-320M-Distill
overwrite_output_dir: true
push_to_hub: true
private: true
# token: <your-token>
hub_model_id: Doge-320M-Distill
hub_strategy: every_save

# Model arguments
model_name_or_path: <your-hub-id>/Doge-320M-Instruct
model_revision: main
trust_remote_code: True
torch_dtype: bfloat16

# Data training arguments
dataset_name: ./datasets/distill_dataset
dataset_configs:
- all
max_seq_length: 4096
packing: true
system_prompt: "As an assistant, you need to thoroughly explore the problem through precise thinking process before providing the final accurate solution. The thinking process includes Analysis, First, Second, Next, Reflection, Finally and Summarizing behavioral steps to develop a well-considered thought process. Please structure your response into two main sections: Thought and Solution. In the Thought section, detail your reasoning process using the specified format: <|begin_of_thought|> {**Analysis:**\\n\\n**First:**\\n\\n**Second:**\\n\\n**Next:**\\n\\n**Reflection:**\\n\\n**Finally:**\\n\\n**Summarizing:**} <|end_of_thought|>. The solution should remain a logical, accurate, concise expression style and detail necessary step needed to reach the conclusion, formatted as follows: <|begin_of_solution|> {**Solution:**} <|end_of_solution|>."

# SFT trainer config
preprocessing_num_workers: 1 # Equal to the number of GPUs you are using
seed: 233
do_train: true
num_train_epochs: 1
per_device_train_batch_size: 1
do_eval: true
eval_strategy: steps
eval_steps: 100
per_device_eval_batch_size: 1
optim: adamw_torch
learning_rate: 2.0e-4
lr_scheduler_type: cosine_with_min_lr
lr_scheduler_kwargs:
  min_lr_rate: 0.1
warmup_ratio: 0.1
weight_decay: 0.01
gradient_accumulation_steps: 128
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false
max_grad_norm: 1.0
bf16: true
