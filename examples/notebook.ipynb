{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doge\n",
    "\n",
    "\n",
    "Train the `Doge` small language model proposed in the paper [Wonderful Matrices](https://arxiv.org/abs/2412.11834).\n",
    "Doge is based on the Transformers framework, replacing the `Multi-Head Attention` in the sequence transformation part with `Dynamic Mask Attention`, and replacing the `MLP` in the state transformation part with `CDMoE`.\n",
    "\n",
    "![doge_architecture](../assets/doge_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PreTraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Pre-Training Datasets\n",
    "\n",
    "\n",
    "For the pre-training dataset, we selected the high-quality text `fineweb-edu-dedup`, the synthetic instruction dataset `cosmopedia-v2`, and supplemented it with `python-edu` and `fine-math` to ensure the model's code and mathematical capabilities.\n",
    "\n",
    "\n",
    "> Note: Due to the large size of the dataset, at least 2TB of storage space is required. If you do not have enough storage space, you can choose to download part of the dataset [here](./utils/download_pt_datasets.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding save path, cache path and number of processes\n",
    "!python ./examples/utils/download_pt_datasets.py --save_dir ./datasets --cache_dir ./cache --num_proc 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Datasets\n",
    "\n",
    "\n",
    "We need to use the `tokenizer` to convert the dataset into `input_ids` and `attention_mask` that the model can accept.\n",
    "If uses the `LlamaTokenizer`, which has a vocabulary size of `32768`, and uses the `[INST]` and `[/INST]` tags to mark instructions. It also includes utility tokens, but we won't use them here.\n",
    "Datasets like cosmopedia-v2 include two fields, `prompt` and `text`, which we will mark as user content and assistant content.\n",
    "\n",
    "```python\n",
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "    {\"role\": \"assistant\", \"content\": text},\n",
    "]\n",
    "return tokenizer.apply_chat_template(conversation, tokenize=True, padding='max_length', truncation=True, max_length=MAX_LENGTH, return_dict=True)\n",
    "```\n",
    "\n",
    "\n",
    "Of course, you can also add some instruction prompts yourself.\n",
    "\n",
    "\n",
    "```python\n",
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I am an AI assistant named `Doge`, I am a language model trained by `Shi Jingze` based on the `Doge` architecture, and my task is to provide appropriate answers and support to users based on their questions and requests.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "    {\"role\": \"assistant\", \"content\": text},\n",
    "]\n",
    "```\n",
    "\n",
    "Here we recommend using the [Doge-tokenizer](https://huggingface.co/SmallDoge/Doge-tokenizer) to process the dataset. It is trained by the `Llama-3.3` tokenizer on the `smollm-corpus`, with a vocabulary size of `32768`. The training script can be found [here](./examples/utils/train_tokenizer_from_old.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding dataset path, save path, tokenizer path, number of samples, max length and number of processes\n",
    "# NOTE: We only keep 256B tokens dataset, the ratio of fineweb-edu:cosmopedia-v2:python-edu:finemath = 7:2:0.5:0.5, if you need to train larger model, please increase the scale of the dataset by yourself\n",
    "!python ./examples/utils/preprocess_pt_datasets.py --datasets_dir ./datasets --save_dir ./datasets --tokenizer_name_or_path SamllDoge/Doge-tokenizer --train_examples 128000000 --test_examples 1000 --max_length 2048 --num_proc 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate Datasets\n",
    "\n",
    "\n",
    "We combine the fineweb-edu_tokenized, cosmopedia-v2, python-edu, and finemath datasets into the `pretraining` dataset.\n",
    "Then shuffle the order `seed=233`, and split out `1,000` samples as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding dataset path, save path, number of samples and number of processes\n",
    "!python ./examples/utils/concatenate_pt_datasets.py --datasets_dir ./datasets --save_dir ./datasets --train_examples 128000000 --test_examples 1000 --num_proc 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Model\n",
    "\n",
    "\n",
    "We configure a `20M` small model for training and testing.\n",
    "\n",
    "| Model | Params | n_layers | d_model | d_ff | n_heads | kv_heads | n_exprets | n_expert_heads | n_expert_pre_head |\n",
    "|---|---|---|---|---|---|---|---|---|---|\n",
    "| Doge-20M | 13M | 8 | 256 | 512 | 2 | 1 | - | - | - |\n",
    "| Doge-60M | 54M | 16 | 512 | 1024 | 4 | 2 | - | - | - |\n",
    "| Doge-160M | 152M | 24 | 768 | 1536 | 6 | 3 | - | - | - |\n",
    "| Doge-320M | 335M | 32 | 1024 | 2048 | 8 | 4 | - | - | - |\n",
    "\n",
    "- n_layers is the number of decoder layers in the model\n",
    "- d_model is the hidden layer dimension of the model\n",
    "- n_heads is the number of heads of multi-head attention, d_model // n_heads is best kept above 64\n",
    "\n",
    "\n",
    "> The `Doge-MoE` model can inherit the dense activation parameters of the `Doge` model, and increase the sparse activation parameters by setting `n_experts`, `n_expert_heads`, `n_expert_pre_head`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Pre-Training Hyperparameters\n",
    "\n",
    "| Model | tokens | max_train_steps | accumulate_steps | learning_rate | scheduler | warmup_ratio | decay_ratio | weight_decay | min_lr_rate |\n",
    "|---|---|---|---|---|---|---|---|---|---|\n",
    "| Doge-20M | 4B | 8,000 | 256 | 8e-3 | warmup_stable_decay | 0.1 | 0.1 | 0.01 | 0.0 |\n",
    "| Doge-60M | 16B | 16,000 | 512 | 6e-3 | warmup_stable_decay | 0.1 | 0.1 | 0.01 | 0.0 |\n",
    "| Doge-160M | 32B | 24,000 | 768 | 4e-3 | warmup_stable_decay | 0.1 | 0.1 | 0.01 | 0.0 |\n",
    "| Doge-320M | 64B | 32,000 | 1024 | 2e-3 | warmup_stable_decay | 0.1 | 0.1 | 0.01 | 0.0 |\n",
    "\n",
    "> According to the experience of [SmolLM blog](https://huggingface.co/blog/smollm), we will scale the parameters in [Chinchilla](https://arxiv.org/pdf/2203.15556) by 10 times the scaling ratio of tokens.\n",
    "\n",
    "> `warmup_stable_decay` is used to continue training with checkpoints on larger datasets at any time, see [Scaling Laws and Compute-Optimal Training Beyond Fixed Training Durations](https://arxiv.org/pdf/2405.18392)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding config path, all arguments are in the config file\n",
    "!ACCELERATE_LOG_LEVEL=info accelerate launch ./src/small_doge/pt.py --config_file recipes/accelerate_configs/single_gpu.yaml --config recipes/doge/Doge-20M/config_full.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage\n",
    "\n",
    "\n",
    "After training is complete, we can use `AutoModelForCausalLM` of `Transformers` to load the model, and use `AutoTokenizer` to load `LlamaTokenizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"SmallDoge/Doge-20M\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"SmallDoge/Doge-20M\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"Hey how are you doing?\", return_tensors=\"pt\")\n",
    "\n",
    "out = model.generate(**inputs, max_new_tokens=20)\n",
    "print(tokenizer.batch_decode(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Fine-Tuning Datasets\n",
    "\n",
    "\n",
    "For the fine-tuning dataset, we selected the `smoltalk` dataset for SFT, and the `ultrafeedback_binarized` dataset for DPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding save path, cache path and number of processes\n",
    "!python ./examples/utils/download_ft_datasets.py --save_dir ./datasets --cache_dir ./cache --num_proc 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Fine-Tuning Datasets\n",
    "\n",
    "\n",
    "We'll apply Fine-Tuning datasets with `chat templete` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding dataset path, save path, tokenizer path, number process.\n",
    "!python ./examples/utils/preprocess_ft_datasets.py --datasets_dir ./datasets --save_dir ./datasets --tokenizer_name_or_path SmallDoge/Doge-tokenizer --num_proc 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate Datasets\n",
    "\n",
    "If you download more datasets for fine-tuning, we need to merge and shuffle them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding dataset path, save path and number of processes\n",
    "!python ./examples/utils/concatenate_ft_datasets.py --datasets_dir ./datasets --save_dir ./datasets --num_proc 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SFT Model\n",
    "\n",
    "We first perform SFT on the model to make it generate responses that follow the `prompt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding config path, all arguments are in the config file\n",
    "!ACCELERATE_LOG_LEVEL=info accelerate launch ./src/small_doge/sft.py --config_file recipes/accelerate_configs/single_gpu.yaml --config recipes/doge/Doge-20M-Instruct/sft/config_full.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DPO Model\n",
    "\n",
    "Then we use reinforcement learning to align SFT model with human preferences, here we use the `DPO` algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding config path, all arguments are in the config file\n",
    "!ACCELERATE_LOG_LEVEL=info accelerate launch ./src/small_doge/dpo.py --config_file recipes/accelerate_configs/single_gpu.yaml --config recipes/doge/Doge-20M-Instruct/dpo/config_full.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, TextStreamer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"SmallDoge/Doge-20M-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"SmallDoge/Doge-20M-Instruct\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(\n",
    "      max_new_tokens=100, \n",
    "      use_cache=True, \n",
    "      do_sample=True, \n",
    "      temperature=0.8, \n",
    "      repetition_penalty=1.0\n",
    ")\n",
    "steamer = TextStreamer(\n",
    "      tokenizer=tokenizer, \n",
    "      skip_prompt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Hi, how are you doing today?\"\n",
    "\n",
    "conversation = [\n",
    "      {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    conversation=conversation,\n",
    "    tokenize=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "outputs = model.generate(\n",
    "    inputs, \n",
    "    tokenizer=tokenizer,\n",
    "    generation_config=generation_config, \n",
    "    streamer=steamer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Refer to [evaluation](../evaluation/README.md)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
